{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing required libraries\n"
      ],
      "metadata": {
        "id": "KgSNHUOcfB4m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SsK0WKJbeCax"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing and reading the corpus\n"
      ],
      "metadata": {
        "id": "Y5qX8hb7fMxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('yugas.txt','r',errors = 'ignore')\n",
        "raw_doc=f.read()\n",
        "raw_doc=raw_doc.lower()#coverts text to lower case\n",
        "nltk.download('punkt')#Using the  punkt tokenizer\n",
        "nltk.download('wordnet')#Using the WordNet dictionary\n",
        "sent_tokens=nltk.sent_tokenize(raw_doc)#coverts doc to list of sentence\n",
        "word_tokens=nltk.word_tokenize(raw_doc)#coverts doc to list of words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJp8wpJde7bC",
        "outputId": "65f42cd3-aa26-4006-f0d1-98a53da3a84b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ceqpL-gIe80X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of sentence tokens\n"
      ],
      "metadata": {
        "id": "Yc59wK4vhPcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpO95tVqhaKG",
        "outputId": "59751ee3-d5da-40a7-9b9f-8d65b59c577e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\naccording to hindu scriptures and mythology, the current universe is destined to pass through four great epochs, each of which is a complete cycle of cosmic creation and destruction.',\n",
              " 'hindu mythology deals with numbers large enough to be nearly impossible to imagine.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of word tokens"
      ],
      "metadata": {
        "id": "GfhMNoiQhlge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xbpm-sUahkXd",
        "outputId": "9d5736ac-bcc6-48a0-9670-6c193e74b5fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['according', 'to']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text preprocessing"
      ],
      "metadata": {
        "id": "BDQKUMFLh0-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer=nltk.stem.WordNetLemmatizer()\n",
        "#wordNet is a semmantically-oriented dictionary of english included in NLTK\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict=dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "E33pNkQyh4Gs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the greeting function"
      ],
      "metadata": {
        "id": "uafmI_UQjdAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GREET_INPUTS=(\"hello\", \"hi\", \"greetings\", \"sup\", \"whats up\", \"hey\")\n",
        "GREET_RESPONSES=[\"hi\", \"hey\", \"nods\", \"hi there\", \"hello\", \"I am glad! you are talking to me\"]\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREET_INPUTS:\n",
        "      return random.choice(GREET_RESPONSES)"
      ],
      "metadata": {
        "id": "jZjP9R80jhK8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response generation"
      ],
      "metadata": {
        "id": "e7f50-02k2Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "wjG9DgUAk5KY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "  robo1_response=''\n",
        "  TfidfVec=TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "  tfidf=TfidfVec.fit_transform(sent_tokens)\n",
        "  vals=cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx=vals.argsort()[0][-2]\n",
        "  flat=vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf=flat[-2]\n",
        "  if(req_tfidf==0):\n",
        "    robo1_response=robo1_response+\"I am sorry ! I don't understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response=robo1_response+sent_tokens[idx]\n",
        "    return robo1_response\n"
      ],
      "metadata": {
        "id": "ay6rnmIulRCq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining converstion start/end protocols"
      ],
      "metadata": {
        "id": "kgU73wQGnse2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag=True\n",
        "print(\"BOT:My name is Ramya.Lets have conversation.Also if you want to exit anytime just type Bye!\")\n",
        "while(flag==True):\n",
        "  user_response=input()\n",
        "  user_response=user_response.lower()\n",
        "  if(user_response!='bye'):\n",
        "    if(user_response=='thanks' or user_response=='thank you'):\n",
        "      flag=False\n",
        "      print(\"BOT:You are welcome\")\n",
        "    else:\n",
        "      if(greet(user_response)!=None):\n",
        "        print(\"BOT: \"+greet(user_response))\n",
        "      else:\n",
        "        sent_tokens.append(user_response)\n",
        "        word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
        "        final_words=list(set(word_tokens))\n",
        "        print(\"BOT: \",end=\"\")\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag=False\n",
        "    print(\"BOT: Good bye! Take care<3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUjbMHz8n0Ov",
        "outputId": "20daeeba-49c0-4a24-a42c-eac42c7408de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT:My name is Ramya.Lets have conversation.Also if you want to exit anytime just type Bye!\n",
            "hello\n",
            "BOT: hello\n",
            "hey\n",
            "BOT: hey\n",
            "whats up\n",
            "BOT: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am sorry ! I don't understand you\n",
            "yugas\n",
            "BOT: time duration of each of the four yugas\n",
            "according to the laws of manu, which was the earliest known text describing the four yugas in detail, the length of each yuga is as follows:\n",
            "\n",
            "4800 years + 3600 years + 2400 years + 1200 years, which equals 12,000 years.\n",
            "treta yuga\n",
            "BOT: yagyas in the treta yuga.\n",
            "kaliyuga\n",
            "BOT: I am sorry ! I don't understand you\n",
            "kali yuga\n",
            "BOT: what is a yuga?\n",
            "types of yugas\n",
            "BOT: consequently people were plagued by ailments, diseases and various types of desires.\n",
            "Symbolic Interpretations\n",
            "BOT: symbolic interpretations\n",
            "metaphorically, the four yuga ages may symbolize the four phases of involution during which the human gradually lost the awareness of his or her inner selves and subtle bodies.\n",
            "Dasavatara\n",
            "BOT: this principle is known as dasavatara (sanskrit dasa = ten).\n",
            "Satya Yuga\n",
            "BOT: satya yuga\n",
            "amongst the four era’s, the satya yuga is the first and the most significant one.\n",
            "Kali Yuga\n",
            "BOT: what is a yuga?\n",
            "bye\n",
            "BOT: Good bye! Take care<3\n"
          ]
        }
      ]
    }
  ]
}